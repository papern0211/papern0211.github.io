I".<h1 id="xception-2017"><a href="https://arxiv.org/pdf/1610.02357.pdf">Xception, 2017</a></h1>

<p>본 논문은 Inception 모듈을 다른 관점에서 해석함으로써, 이후 Convolution layer 경량화에 많이 사용되는  <strong>Depthwise seperable convolution</strong> (Depthwise convolution + pointwise convolution)과의 연관성을 설명하고, 동일한 파라미터 크기를 모델로써 Inception V3보다 더 나은 성능을 도출하였다.</p>

<p>아래 그림에서 보면, Inception module의 간단 버젼 [Fig. 1-(a)] 에서 3x3 convolution으로 통일시키고 및 Avg. Pool을 제거한 뒤 [Fig. 1-(b)], 1x1 convolution을 개념적으로 하나로 통일해서 생각하면 [Fig. 1-(c)], 이후에 적용되는 3x3 convolution는 output channel간의 겹치지 않고, 독립적으로 동작한다고 볼 수 있다. 만약 극단적으로 모든 채널에 대해 spatial correlation을 분리해서 본다면 [Fig. 1-(d)], 이는 결국 <strong>Depthwise separable convolution</strong> 와 거의 같은 형태를 띄게 된다.
&lt;/br&gt;&lt;/br&gt;
<img src="../assets/images/2020-12-20-Xception/inception.jpg" alt="Fig. 1. Inception 모듈의 변경 및 해석" />
&lt;/br&gt;&lt;/br&gt;
결국, 극단적으로 Inception 모듈을 구현 하였다고 볼 수 있어서 (extreme inception), 제안하는 모델 구조를 <strong>Xception</strong> 이라고 부르게 된다.</p>

<p>Depthwise seperable convolution과는 두가지 관점에서 약간 차이가 있는데, 다음과 같다.</p>
<ol>
  <li>연산 순서</li>
  <li>비선형 연산 존재 유무</li>
</ol>

<p>논문에서는 연산순서의 경우 크게 고민을 하지 않았는데, 그 이유는 우리가 모델을 구성할때, 여러 모듈을 겹겹히 쌓게 되고 자연히 1x1-&gt;3x3-&gt;1x1-&gt;3x3…의 순서가 나타나게 되서 큰 차이가 없다고 판단했다.</p>

<p>하지만, 비선형 연산의 유무의 경우 두모듈에서  큰 차이점을 보여주게 되는데, 비선형을 제거할 수록, 다시 말해 ReLU연산을 제거 하면, 더 좋은 성능을 얻을 수 있게 되었다. 이는 Szegedy 가 주장과 상반된 결과 인데, 본 논문에서는 그 차이가 feature space의 깊이 (feature space의 채널 크기)에  인한 것으로 생각된다고 말한다 (Depthwise seperable convolution은 깊이가 1)
&lt;/br&gt;&lt;/br&gt;
<img src="../assets/images/2020-12-20-Xception/nonlinearity.jpg" alt="Fig. 2. 비선형 activation에 따른 성능" />
&lt;/br&gt;&lt;/br&gt;</p>

<p>기본적으로 Depthwise seperable layer을 겹겹히 쌓고, 더불어 residual connection을 추가하였으며, Entry flow/Middle flow/Exit flow 세가지 모듈을 이용해 아키텍쳐를 구성하였다. 성능은 당연히 비교 대상인 Inception V3 보다 잘 나왔는데, ImageNet 데이터 [Fig. 3] 결과와 더불어 JFT 데이터 결과에서 모두 나은 정확도를 보여줬다.
&lt;/br&gt;&lt;/br&gt;
<img src="../assets/images/2020-12-20-Xception/results.jpg" alt="Fig. 3. ImageNet 데이터에서 모델별 성능" />
&lt;/br&gt;&lt;/br&gt;</p>
:ET