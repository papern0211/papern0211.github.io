var store = [{
        "title": "[논문 리뷰] Xception: Deep Learning with Depthwise Separable Convolutions",
        "excerpt":"Xception 2016 Overview 본 논문은 Inception 모듈을 다른 관점에서 해석함으로써, 이후 Convolution layer 경량화에 많이 사용되는 Depthwise seperable convolution (Depthwise convolution + pointwise convolution)과의 연관성을 설명하고, Inception V3과 동일한 파라미터 크기를 가지는 모델을 이용해 더 나은 성능을 도출하였다. Method and analysis 아래그림에 Inception 모듈의 기본 버젼 [Fig. 1-(a)]에서 $3 \\times...","categories": ["Deep Learning"],
        "tags": ["CNN","Vision"],
        "url": "http://localhost:4000/deep%20learning/Xception/",
        "teaser": "http://localhost:4000/assets/images/head_image.jpg"
      },{
        "title": "[논문 리뷰] MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications",
        "excerpt":"MobileNets, 2017 2020년 12월 기준 인용수가 무려 6,000이 넘을 정도로 많은 연구자들이 인용한 논문으로써, 향후 많은 논문들에서 채택한 Depthwise seperable layer을 이용해 경량화를 효율적으로 보여준 연구이다. 앞서 언급했듯이, MobileNets 에서 경량화의 핵심은 바로 Depthwise seperable convolution으로 아래 그림에서와 같이 Batch Normalization과 ReLU을 같이 조합해 구성되었다. Fig. 1. 일반적인 convolution과 depthwise...","categories": ["Deep Learning"],
        "tags": ["CNN","Vision"],
        "url": "http://localhost:4000/deep%20learning/MobileNet/",
        "teaser": "http://localhost:4000/assets/images/head_image.jpg"
      },{
        "title": "[논문 리뷰] MobileNetV2: Inverted Residuals and Linear Bottlenecks",
        "excerpt":"MobileNets V2, 2018 2017년에 이어 구글 개발진은, MobileNets v2 을 발표하면서 경량화 관점에서 더 최적화된 구조를 제안하였다. MobileNets v1에서 핵심인 Depthwise seperable convolution은 여전히 사용하는 대신, 구조적인 면에서 새로운 개념을 제시하였다. Linear bottlencks Inverted residuals 우선 Linear bottlenecks 구조는 Covolution layer 구조 설계시 당연시하게 사용되는 ReLU 연산에 대한 고찰에서 출발하였다....","categories": ["Deep Learning"],
        "tags": ["CNN","Vision"],
        "url": "http://localhost:4000/deep%20learning/MobileNet_V2/",
        "teaser": "http://localhost:4000/assets/images/head_image.jpg"
      }]
