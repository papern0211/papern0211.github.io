---
title:  "[논문 리뷰] Improving Language Understanding
by Generative Pre-Training (OpenAI GPT-1)"
excerpt: "Improving Language Understanding
by Generative Pre-Training"

categories:
  - Deep Learning
tags:
  - RNN
  - Transformer
  - NLU
classes: wide
last_modified_at: 2020-12-27T00:10:52+09:00
---

__[paper link](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)__  

대부분 실생활 딥러닝 문제에서 잘 정리된 labeled 데이터를 얻는 것은 막대한 시간과 비용을 요구한다. 이는 Natural language processing (NLP) 분야에서도 크게 다르지 않는데, 막대한 unlabeled 데이터가 온/오프라인상에 존재함에도 불구하고 우리는 그것을 제대로 활용하지 못하고 있다.

본 논문에서는 이러한 문제의 해결법으로 unlabeled raw text로 부터 task에 국한되지 않는 representation을 모델을 제시하고, 이를 이용해 다양한 분야의 NLP task에서 성능 향상을 이룰 수 있음을 보여주었다.

## __Framework__
학습은 두가지 단계로 구성된다.
- Unsupervised pre-training
- Supervised fine-tunning

그리고 Task에 따라 입력의 transformation이 선행적으로 필요한 경우가 존재한다.

### 1. Unsupervised pre-training
대량의 corpus로 부터, 다음의 likelihood을 최대하 하도록 모델이 학습된다.
$$ L_{1}(\mathcal{U})=\sum_{i}\log P(u_i | u_{i-k}, ..., u_{i-1}; \Theta) $$
여기서 $\mathcal{U}=\{u_1, ..., u_n\}$는 corpus을, $k$는 context window의 크기를 의미한다.

GPT에서는 language 모델 구성을 위해, __multi-layer Transformer decoder__ 을 이용한다. 입력 토큰에 대해 multi-headed self-attention을 적용하고, 해당 출력에 대해 point-wise feedforward layer을 추가하여, 출력 확률을 계산한다.
$$ h_0 = UW_e + W_p $$ 
$$ h_l = transformer\_block(h_{l-1})\quad \forall i\in[1,n] $$
$$ P(u) = softmax(h_n W^{T}_e) $$
여기서 $U=(u_{-k},...,u_{-1})$은 token vector, $n$은 layer 갯수, $W_e$는 token embedding matrix 그리고 $W_p$ 는 position embedding matrix을 의미한다.

### 2. Supervised fine-tunning
Unsupervised pre-training을 통해 모델을 학습한뒤, supervised target task에 대해 모델 파라미터를 adaptation 한다. 앞선 모델의 마지막 transformer의 activation에 단순히 linear output layer을 추가하여 주어진 입력 token의 sequence에 대해 prediction을 하고, 이 확률을 최대화 한다. (단순하게 생각해서 linear layer 하나 더 추가해서 classfication 문제 푸는거다!)
$$ P(y|x^1, ..., x^m)= softmax(h_l^mW_y) $$
$$ L_2(\mathcal{C})=\sum_{(x, y)} \log P(y|x^1,...,x^m) $$
여기서 $\mathcal{C}$는 labeled dataset, $x^1, ... x^m$은 입력 token sequence, $y$ 는 label을 의마한다.

여기에 더해 auxiliary objective을 포함시킬 수 있는데, 다음과 같다
$$ L_3(\mathcal{C}) = L_2(\mathcal{C}) + \lambda * L_1(\mathcal{C})$$
이러한 auxiliary objective는 학습된 모델의 일반화에 보다 도움을 주고, 또한 학습시 수렴 속도 개선을 도와준다고 본 논문에서는 주장한다.

### 3. Task-specific input transformations
일반적으로, text classification 같은 문제의 경우 특별히 모델에 변형을 가할 필요는 없다. 하지만, question answering 이나 textual entailment 같은 경우는 문장의 순서, 질문/답변/문서간의 pair등, 입력을 묶어서 구조화된 형태로 사용해야 되는 경우가 있다. 

이를 위해 모델의 구조를 바꾸는 것은, 번거로울 뿐만 아니라, 실제 pre-trained 모델에서 얻은 결과가 제대로 전이되지 못하는 문제가 발생가능하다. 본 연구에서는 traversal-style 접근을 이용했는데, 이는 이러한 구조화된 형태의 데이터를 하나의 sequence로 표현하여 pre-trained 모델에서 처리할 수 있도록 하였다. 이를 위해 __[start]__, __[end]__, 그리고 __[delim.]__ 토큰이 추가되었고, 모델에서 이러한 부분을 캐치할 수 있도록 학습시 사용하였다.

## __Experiments__
GPT는 language model을 학습하기위해 BooksCorpus dataset을 이용하였다. 다양한 장르의 약 7000개의 미출판 책이 포함된 이 데이터에는 상당히 긴 문장도 포함되어 있는데, transformer 기반의 구조에서 긴 범위의 문장의 의미를 학습할 수 있기에 좋은 데이터 셋이였다. (참고로 ELMo에서 사용한 1B
Word Benchmark의 경우 비슷한 규모의 데이터지만, 문장단위로 shuffle이 되어 있다보니, 이러한 긴 문장을 커버하기에는 적합하지 못했다.)

Pre-trained 모델 구조 및 학습 파라미터는 다음과 같다
- 12 layer decoder-only transformer (768-d, 12 heads)
- 3072-d point-wise feedforward network
- Adam optimizer: max learning rate of 2.5e-4
- The learning rate 는 처음 2000 update까지는 선형적으로 증가하다 이후, cosine schedule을 이용해 0으로 서서히 감소
- batch size: 64, epoch: 100
- number of tokens in sequence: 512
- layer normalization 사용
- 0.1의 atttention dropout
- GELU (Gaussian Error Linear Unit) 사용
- 공백과 구두점 해결을 위해 전처리 작업 진행  

Fine-tunning의 경우, 댜른 파라미터를 적용하였는데, 다음과 같다
- batch size: 32, epoch: ~3
- learning rate: 6.25e-5, linear rate decay
- 0.1의 classifier dropout

다양한 task에 대해서 대부분 SOTA 결과를 얻었다. 정확도 수치에 대해서는 논문을 참고하면 된다.

## __Analysis__
### 1. Impact of number of layers transferred
본 연구에서는, pre-trained layer의 전이 과정이 정말로 효과적으로 진행될 수 있는지를 확인하기 위해 layer의 크기를 변경해가면서, 실제 전이가 잘 이루어지는 지를 확인하였다. 

아래 그림에서 확인 가능하듯, 전이되는 layer 갯수가 늘릴 수록 정확도는 늘어남을 확인 할 수 있었다. 단순히 일반화 하기는 어려울 수 있겠지만, pre-trained 모델의 각각의 layer가 target task 문제를 풀어나가는데 있어서, 유용한 정보를 포함하고 있다고 생각 할 수 있겠다.
![](/assets/images/2020-12-27-GPT1/performance.jpg)

### 2. Zero-shot Behaviors
Transformer 기반의 pre-trained model이 왜 효과적인지를 분석하였다. 이를 위해 LSTM 기반의 방법과 비교하여 다양한 task에 대해 성능을 비교하였는데, transformer 기반의 방법이 훨씬 효과적임을 확인 할 수 있다. 아마 transformer의 경우 구조적인 특징으로 인해 보다 많은 attention 정보가 학습 전이하는데 도움을 줄 수 있었을 것으로 판단된다.

## __Comment__
GPT 모델의 처음 버젼으로 pre-trained model과 Supervised fine-tunning 을 통해 NLP의 다양한 task에서 좋은 성능을 낼 수 있음을 보여준 연구이다. 사실 본 포스트를 정리하는 시점에 이미 GPT-3 까지 나와있는 상황이라 history을 본다는 관점에서 다시금 정리한 논문이라, 막상 '와~' 하는 내용은 아니였지만, 이후 BERT와 함께 pre-trained model 연구에 큰 획을 그었다는 점에서 그 의미를 찾을 수 있다. 




