---
title:  "[논문 리뷰] Xception: Deep Learning with Depthwise Separable Convolutions"
excerpt: "Xception: Deep Learning with Depthwise Separable Convolutions"

categories:
  - Deep Learning
tags:
  - CNN
  - Vision
classes: wide
last_modified_at: 2020-12-21T00:21:00+09:00
---
__[arxiv link](https://arxiv.org/pdf/1610.02357.pdf)__

## Overview
본 논문은 Inception 모듈을 다른 관점에서 해석함으로써, 이후 Convolution layer 경량화에 많이 사용되는  __Depthwise seperable convolution__ (Depthwise convolution + pointwise convolution)과의 연관성을 설명하고, Inception V3과 동일한 파라미터 크기를 가지는 모델을 이용해 더 나은 성능을 도출하였다. 

## Method and analysis
아래그림에 Inception 모듈의 기본 버젼 [Fig. 1-(a)]에서 $3 \times 3$ convolution으로 통일시키고, Avg. Pool을 제거한 뒤 [Fig. 1-(b)], $1 \times 1$ convolution을 개념적으로 하나로 통일하면 [Fig. 1-(c)], $3 \times 3$ convolution는 출력 채널간의 겹치지 않고 독립적으로 동작한다. 만약 극단적으로 모든 채널에 대해 spatial correlation을 분리해서 본다면 [Fig. 1-(d)], 이는 결국 __Depthwise separable convolution__ 와 거의 같은 형태를 띄게 된다.

{% capture fig1 %}
![Foo]({{ "/assets/images/2020-12-20-Xception/inception.jpg" | relative_url }})
{% endcapture %}

<figure>
  {{ fig1 | markdownify | remove: "<p>" | remove: "</p>" }}
  <figcaption>Fig. 1. Inception 모듈의 변경 및 해석</figcaption>
</figure>

이는 극단적으로 Inception 모듈을 구현 하였다고 볼 수 있어서 (extreme inception), 저자들은 위 모델을 __Xception__ 이라 부르게 된다. 

Depthwise seperable convolution과는 두가지 관점에서 약간 차이가 있는데 다음과 같다. 
1. 연산 순서
1. 비선형 연산 존재 유무  

논문에서는 연산순서의 경우 크게 고민을 하지 않았는데, 그 이유는 우리가 모델을 구성할때, 여러 모듈을 겹겹히 쌓게 되고 자연히 $1 \times 1$→$3 \times 3$→$1 \times 1$→$3 \times 3$...의 순서가 나타나게 되서 큰 차이가 없다고 판단했다.

하지만, 비선형 연산 유무 여부를 두고 두 모델을 비교해보면 큰 차이점이 있는데, 비선형을 제거할수록(즉, ReLU연산을 제거 할수록) 더 좋은 성능을 얻게 된다. 이는 Szegedy 가 주장과 상반된 결과 인데, 본 논문에서는 그 차이가 feature space의 깊이 (feature space의 채널 크기)에 의한 것으로 생각된다고 말한다 (Depthwise seperable convolution은 깊이가 1)

{% capture fig2 %}
![Foo]({{ "/assets/images/2020-12-20-Xception/nonlinearity.jpg" | relative_url }})
{% endcapture %}

<figure>
  {{ fig2 | markdownify | remove: "<p>" | remove: "</p>" }}
  <figcaption>Fig. 2. 비선형 activation에 따른 성능</figcaption>
</figure>

모델은 Depthwise seperable layer을 겹겹히 쌓고, residual connection을 추가하였으며, Entry flow/Middle flow/Exit flow 세가지 모듈을 이용해 아키텍쳐를 구성하였다. 성능은 당연히 비교 대상인 Inception V3 보다 잘 나왔는데, ImageNet 데이터 [Fig. 3] 뿐만 아니라 JFT 데이터에서도 나은 정확도를 보여줬다.

{% capture fig3 %}
![Foo]({{ "/assets/images/2020-12-20-Xception/results.jpg" | relative_url }})
{% endcapture %}

<figure>
  {{ fig3 | markdownify | remove: "<p>" | remove: "</p>" }}
  <figcaption>Fig. 3. ImageNet 데이터에서 모델별 성능</figcaption>
</figure>

