---
title:  "[논문 리뷰] Xception: Deep Learning with Depthwise Separable Convolutions"
excerpt: "Xception: Deep Learning with Depthwise Separable Convolutions"

categories:
  - Deep Learning
tags:
  - CNN, Vision
last_modified_at: 2020-12-20T20:00:00+09:00
---
# [Xception, 2017](https://arxiv.org/pdf/1610.02357.pdf)

본 논문은 Inception 모듈을 다른 관점에서 해석함으로써, 이후 Convolution layer 경량화에 많이 사용되는  __Depthwise seperable convolution__ (Depthwise convolution + pointwise convolution)과의 연관성을 설명하고, 동일한 파라미터 크기를 모델로써 Inception V3보다 더 나은 성능을 도출하였다. 

아래 그림에서 보면, Inception module의 간단 버젼 [Fig. 1-(a)] 에서 3x3 convolution으로 통일시키고 및 Avg. Pool을 제거한 뒤 [Fig. 1-(b)], 1x1 convolution을 개념적으로 하나로 통일해서 생각하면 [Fig. 1-(c)], 이후에 적용되는 3x3 convolution는 output channel간의 겹치지 않고, 독립적으로 동작한다고 볼 수 있다. 만약 극단적으로 모든 채널에 대해 spatial correlation을 분리해서 본다면 [Fig. 1-(d)], 이는 결국 __Depthwise separable convolution__ 와 거의 같은 형태를 띄게 된다.
</br></br>
![Fig. 1. Inception 모듈의 변경 및 해석](/assets/images/2020-12-20-Xception/inception.jpg)
</br></br>
결국, 극단적으로 Inception 모듈을 구현 하였다고 볼 수 있어서 (extreme inception), 제안하는 모델 구조를 __Xception__ 이라고 부르게 된다.

Depthwise seperable convolution과는 두가지 관점에서 약간 차이가 있는데, 다음과 같다. 
1. 연산 순서
1. 비선형 연산 존재 유무  

논문에서는 연산순서의 경우 크게 고민을 하지 않았는데, 그 이유는 우리가 모델을 구성할때, 여러 모듈을 겹겹히 쌓게 되고 자연히 1x1->3x3->1x1->3x3...의 순서가 나타나게 되서 큰 차이가 없다고 판단했다.

하지만, 비선형 연산의 유무의 경우 두모듈에서  큰 차이점을 보여주게 되는데, 비선형을 제거할 수록, 다시 말해 ReLU연산을 제거 하면, 더 좋은 성능을 얻을 수 있게 되었다. 이는 Szegedy 가 주장과 상반된 결과 인데, 본 논문에서는 그 차이가 feature space의 깊이 (feature space의 채널 크기)에  인한 것으로 생각된다고 말한다 (Depthwise seperable convolution은 깊이가 1)
</br></br>
![Fig. 2. 비선형 activation에 따른 성능](/assets/images/2020-12-20-Xception/nonlinearity.jpg)
</br></br>

기본적으로 Depthwise seperable layer을 겹겹히 쌓고, 더불어 residual connection을 추가하였으며, Entry flow/Middle flow/Exit flow 세가지 모듈을 이용해 아키텍쳐를 구성하였다. 성능은 당연히 비교 대상인 Inception V3 보다 잘 나왔는데, ImageNet 데이터 [Fig. 3] 결과와 더불어 JFT 데이터 결과에서 모두 나은 정확도를 보여줬다.
</br></br>
![Fig. 3. ImageNet 데이터에서 모델별 성능](/assets/images/2020-12-20-Xception/results.jpg)
</br></br>
