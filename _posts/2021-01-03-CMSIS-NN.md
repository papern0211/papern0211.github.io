---
title:  "[논문 리뷰] CMSIS-NN: Efficient Neural Network Kernels for ARM Cortex-M CPUs"
excerpt: "CMSIS-NN: Efficient Neural Network Kernels for ARM Cortex-M CPUs"

categories:
  - Deep Learning
tags:
  - Deep learning compiler
classes: wide
last_modified_at: 2021-01-03T10:00:00+09:00
---
__[arxiv link](https://arxiv.org/pdf/1801.06601.pdf)__  

ARM Cortex-M CPUs에 특화된 Neural Network 최적화 커널이다.
커널 코드는 크게 다음의 두가지로 구분되는데, 다음과 같다.
- NN Functions
  - Convolution
  - Depswise seperable convolution
  - Fully-connected network
  - Pooling
  - Activation
- NN Support Functions
  - Utility functions: data conversion, activation function tables
  - LSTM 이나 GRM 같은 복잡한 NN 모듈을 구성하는데 사용

커널을 소개하기 전에 우선, Fixed-point quantization에 대해서 알아보자. 대부분 Inference 작업에서 32bit-floating point 변수의 사용은 불필요하다. Fixed-point 최적화를 통해 연산 부담을 줄일 수 있을 뿐만 아니라, 메모리 접근 횟수도 줄임으로써 MCU와 같은 제한된 리소스를 가지는 플랫폼에서는 거의 필수적으로 적용해야 된다.

현재 8BIT, 16bit Fixed-point 커널을 지원하는데, CMSIS에서 사용되는 방석으로 데이터를 표현한다. 즉, _int8_ 은 _q7_t_ 로, _int16_ 은 _q16_t_ 로 그리고 _int32_ 는 _q32_t_ 로 표현한다. 이러한 방싁은 fixed-point 데이터를 2의 거듭제곱 형태로 스케일링 하는 것을 기본 가정으로 하는데, 이를 위해 적절한 bitwise shift 연산을 적용하여 quantization 한다.

Cortex-M 계열의 프로세서들은 32-bit RISC 프로세서 코어이다. CMSIS-NN에서는 16-bit Multiply-and-Accumulate (MAC) instruction와 같이 NN 계산을 하는데, SIMD Instruction 활용에 중점을 두었다. 

## __Support Functions__
16-bit MAC을 사용하기 위해서는 8-bit 데이터를 16-bit 데이터로 변환하는 작업이 요구된다. CMSIS 에서는 _arm_q7_to_q15_ 을 제공하는데, 아래 그림과 같이 2단계로 동작한다.
1. 8-bit 데이터를 16-bit 데이터로 확장. 이때, __SXTB16 연산을 이용 (sign extestion)
1. 확장된 데이터를 입력과 같은 순서로 정렬

![CMSIS _arm_q7_to_q15_, figure3](/assets/images/2021-01-03-CMSIS-NN/figure3.jpg)

데이터 변환은 실제 성능에 매우 많은 영향을 끼치는 요소로써, 이를 최적화 하는 것은 매우 중요하다. 위의 첫번째 요소는 필수적인 부분이라 어떻게 할 수 없지만, 두번째 부분은 사용하는 operand들이 모두 같은 순서만 가질 수 있다면 제거 가능하다. CMSIS-NN에서는 순서 정렬하는 부분을 제거한 데이터 변환을 이용한다.

## __Matrix Multiplication__
CMSIS에서 사용된 _mat_mult_ 커널 기반으로 구현되었다. Matrix Multiplication은 $2 \times 2$ 커널 형태로 구성되는데, 이러한 방법을 통해 __데이터 재사용__ 와 __load instruction 절감__ 을 가능케 했다. Accumulation은 _q32_t_, _q16_t_ 모두 지원하고, 초기값으로 bias을 미리 넣어 세팅해 놓는다.

![CMSIS matrix multiplication, figure5](/assets/images/2021-01-03-CMSIS-NN/figure5.jpg)

비슷한 방법으로 matrix vector 커널도 구성되었는데, $1 \times 2$ 커널 형태로 구성되고 동작 방식은 같다. NN inference 과정에서 weight는 사수값이며, 재사용 가능한 부분이기에 weight re-ordering을 해주어, 앞서 support functions에서 언급한 데이터 순서 정렬 부분을 생략할 수 있게 된다. Weight re-ordering과 실제 연산 과정은 아래 그림에 나타나 있다.

![CMSIS-NN weight reordering, figure6](/assets/images/2021-01-03-CMSIS-NN/figure6.jpg)
![CMSIS-NN MAC, figure7](/assets/images/2021-01-03-CMSIS-NN/figure7.jpg)

## __Convolution__
Convlution 연산은 일반젖ㄱ으로 입력을 re-ordering하고 expanding 하는 작업을 거친 후, matrix multiplication 연산을 수행한다. 이 변환 작업을 _im2col_ 이라고 하는데, 이 _im2col_ 과정에서 중복되서 같은 픽셀 값을 접근하게 되어 메모리 footprint 횟수가 늘어나게 된다.

CMSIS-NN 에서는 HWS(height-width-channel)-style 레이아웃을 채택해 데이터 이동을 효율적으로 가능케 했다. 즉, 같은 픽셀(x,y 좌표가 같은)은 연속적으로 저장하게 하고 SIMD 연산을 통해 효율적으로 복사/이동이 이루어지게 하였다.

## __Pooling__
Convolution 연산과 다르게 Pooling은 같은 채널안에서 연산이 이루어진다. 즉, 채널간 독립적이다. 보통 Avg. Pooling과 Max Pooling이 사용되는데, 가장 원시적으로는 주어진 kernel 내에 돌아가면서 연산 혹은 비교를 수행하게 된다. CMSIS-NN에서는 좀 더 효율적으로 이를 구현하기 위해 width와 height에 따라 x-pooling과 y-pooling으로 나눠 진행하였다. 이러한 방법을 통해 x방향으로 진행된 max/avg. operation을 재활용 할 수 있게 되어 실질적으로 적용되는 operation 갯수를 절약할 수 있게 되었다.

## __Activation functions__
ReLU 연산은 SWAR (SIMD with register)와 비슷한 컨셉으로 구현되었다. 핵심은 _q7_t_ 에서 MSB (부호)를 bit shift을 이용해 LSB로 옮기고, byte-level 뺄셈을 이용해 마스크를 만든 뒤, 원래 데이터에 적용하는 것이다. 이러한 방법을 사용하면 일반적인 방법 대비 약 4배 빠른 속도 향상을 달성할 수 있었다.

![CMSIS-NN MAC, figure12](/assets/images/2021-01-03-CMSIS-NN/figure12.jpg)

Sigmoid와 tanh의 경우는 RNN 계열의 모델에서 자주 쓰이는데, table-lookup 접근 방법을 이용해 쉽게 구현하였다. 입력 값의 MSB을 이용해 lookup table의 entry을 찾고 (hash 역할), LSB는 선형 보간을 통해 근사화 하였다.

CIFAR-10 데이터 셋에서 CMSIS-NN 커널은 기존 caffe 기반 커널보다 CNN 기준 약 4.6배 runtime/throughput 및 약 4.9배 에너지 효율을 높였고, 상용화 ARM Cortex-M7 MCU 테스트 시, 초당 10.1개 이미지 처리를 가능케 했으며, 이때 약 79.9% 정확도를 달성하였다.

