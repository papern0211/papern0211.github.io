---
title:  "[논문 리뷰] MCUNet: Tiny Deep Learning on IoT Device"
excerpt: "MCUNet: Tiny Deep Learning on IoT Device"

categories:
  - Deep Learning
tags:
  - NAS
  - Deep learning compiler
classes: wide
last_modified_at: 2020-12-31T10:00:00+09:00
---
__[arxiv link](https://arxiv.org/pdf/2007.10319.pdf)__  

효율적인 neural architecture (TinyNAS)와 가벼운 inference 엔진 (TinyEngine)을 제안하여, ImageNet-scale inference을 micro-controller (MCU)에서 동작하도록 하였다.

TinyNAS는 2-stage NAS 접근 방법을 사용하였는데, 다음과 같다
- Resource 제약에 맞게, search space을 우선 최적화
- 최적화된 search space에서 network architecture을 특정화

이를 통해 device, latency, energy, 메모리등 다양한 변수를 적은 search 비용으로 최적화 하였다.

TinyNAS와 연계되어 설계된 TinyEngine은 메모리를 효율적으로 사용하도록 고안된 inference 엔진으로, layer단위의 최적화가 아닌 전체 network topology단에서의 메모리 스케쥴링 최적화를 하였다. 이를 통해 TF-lite micro, CMSIS-NN 보다 3.4배의 메모리 사용량 감소 및 1.7-3.3배의 속도 향상을 이루어 냈다.

MCUNet은 상용 MCU에서 최초로 ImageNet top-1 정확도를 70%이상 도출한 프레임워크로써, Quantized MobileNet V2와 ResNet-18보다 3.5배 적은 SRAM, 5.7배 적은 Flash 메모리를 사용하였다. 그리고 MobileNet V2와 Proxyless NAS 기반 솔루션보다 각각 2.4-3.4배 속도 향상을 이뤄냈고, 3.7-4.1배 적은 peak SRAM 사용을 달성하였다.

MCUNet에 대해 논문에 기술된 순서대로 자세히 알아보자

## __Introduction__
일반적으로 MCU에서 on-chip 메모리는 매우 열악하다.
- Mobile device 대비 3 order 작은 수준
- Cloud GPUs 대비 5 order 작은 수준

예를 들어, 최신 ARM Cortex-M7 MCU는 320kB SRAM, 1MB Flash 스토리지 정도 가지고 있다. ResNet-50을 구동하기 위해서는 100배, MobileNet V2는 22배, 심지어는 int8 quantized MobileNet V2 또한 5.3배 이상의 메모리 크기를 요구하는데, 상당한 갭이 존재한다.

기존의 연구들을 대부분, GPU나 혹은 스마트폰에 맞춰 효율적인 네트워크 설계와 NAS 방법을 제안했는데, 이는 여전히 MCU보다는 여유로운 상황이였다. 예를 들어,
- MobileNet V2 의 경우 ResNet-18 보다 4.6배나 작은 수준의 메모리를 사용하고, ImageNet 문제에 대해 70% 수즌의 정확도를 보여줬지만, peak activation 크기가 오히려 1.8배 증가하였고, MCU의 SRAM에 맞추기가 어려웠다.
- MUC에 머신러닝을 적용한 연구는 몇몇 존재하지만, 대부분 작은 데이터 셋(CIFAR) 수준으로, 실생활에 적용하기에 거리가 있다.

본 논문에서는 효율적은 Neural architecture (TinyNAS)와 가벼운 Inference 엔진 (TinyEngine)을 제안하여, ImageNet 스케일 inference을 MCU에서 수행 가능케 하였다.

TinyNAS는 2단계 NAS 접근 방법을 채택했다.
- 리소스 제약에 맞게, search space을 우선 최적화
- 최적화된 search space에서 network architecture을 특정화

TinyNAS는 우선 input resolution과 model width 를 변경함으로써, 다양한 search space을 생성하고, 이중 우선순위를 평가하기 위해 FLOPs 분포를 계산했다. 기본적으로 가지고 있는 생각은 __"동일한 메모리 제약내에서 FLOPs이 높을 수록 좋은 모델을 생성한다"__ 이다

TinyNAS와 연계되어 설계된 TinyEngine은 메모리를 효율적으로 사용하게 한 inference 엔진으로 layer 단위의 최적화가 아닌 전체 network topology단에서 메모리 스케쥴링을 최적화 하였다. 최종적으로 각 레이어 마다 특화된 커널 최적화 (loop tiling, loop unrolling, op fusion)을 적용하여, inference 속도를 향항 시킬 수 있었다.

성능은 아래와 같이 달성하였다.
- TF-lite/CMSIS-NN  대비 3.4배 peak 메모리 사용량 감소, 1.7-3.3배 속도 향상
- 상용 MCU에서 ImageNet task top-1 정확도 70.7% 달성
- Visual/Audio wakeword task의 경우 SOTA 성능(?)을 달성하였고, 현재 솔루션 대비 2.4-3.4배 속도 향상 그리고 peak SRAM을 3.7-4.1배 작게 사용
- Speech command dataset에 대해 91% top-1 정확도 및 10FPS 달성

## __Background__
MCU는 매우 타이트한 메모리를 가지고 있다. 딥러닝 시나리오에서 타이트한 SRAM (read & write)과 Flash (read only) 크기는 각각 activation 크기와 모델 크기를 결정하는데 있어 큰 제약이 된다. 두가지 관점에서 살펴봐야 되는데 다음과 같다.
- Deep learning inference on MCUs  
TF-lite micro, CMSIS-NN, CMix-NN, MicroTVM과 같은 MCU에 적용가능한 프레임워크가 있지만, 대부분의 경우 런타임시 네트워크 그래프를 해석하고 동작하다 보니, interpreter의 성능에 영향을 받게 된다. 이는 곧 SRAM과 Flash 메모리 사용량 및 latency 증가를 야기한다. 또한 layer 레벨에서만 최적화가 진행되다 보니, 네트워크 전반에 걸친 메모리 최적화가 이뤄지지 않는다.
- Efficient neural network design  
효율적인 네트워크를 디자인하기 위해, prunning, quantization 그리고 tensor decomposition과 같은 방법을 적용할 수 있다. 또한 neural architecture search (NAS) 방법이 존재하는데, NAS의 경우 search space을 어떻게 설정하느냐에 따라 성능에 많은 영향을 준다. 이를 위해 현재 가장 주목받고 있는 모델의 구조로 부터 hueristic 방법으로 search space을 설정하여 NAS 모델을 만들어 내는 방법이 존재하지만, 이를 위해서는 매뉴얼한 튜닝 작업이 요구되고, 결국 deployment에 많은 제약을 만들게 된다.

## __MCUNet__
기존에는 다음과 같이 최적화가 이뤄지는데,
- 주어진 라이브러리 기반으로 NN을 최적화 (모델 최적화)
- 주어진 NN에서 라이브러리 튜닝을 통하 최적화 (Inference 최적화)

MCUNet에서는 두가지 작업을 하나의 loop안에 구현을 해서 jointly 최적화하는 프레임워크를 제공한다.

### __TinyNAS: Two-Stage NAS for Tiny Memory Constraints__
TinyNAS는 2단계 구조로 동작한다.
#### 1. Automated search space optimiztion  
다양한 MCU의 리소스 환경에 맞추기 위해, 모델 search space에서 다양한 스케일의 _image resolution_ 과 _width multiplier_을 사용한다. 이들의 조합을 통해 search space가 구성되고, 각각의 search space는 또한 천문학적인 sub-networks로 구성된다. 

모든 경우에 대해 각각의 결과를 도출해서 비교해 본다는 것은 사실상 불가능하다. 이에 저자들은 각각의 search space 별로 약 m개의 sub-network를 임의로 추출하여 평가하는 방법을 채택하였다. 여기에 더해 각 network의 정확도를 기준으로 CDF (cumulative distribution function)을 계산하는 것이 아닌, network의 FLOPs 기준으로 CDF을 계산하였다. 이는 같은 모델 구조에서 __계산량이 크면 곧 모델의 Capacitiy가 좋아지고 이는 정확도 향상을 가져온다__ 라는 가정에 기반한다.  

실제 ARM Cortex-M7 MCU STM32F746에 ImageNet-100 데이터에 대해 search space을 찾아보면, 아래 그림과 같은 결과를 확인 할 수 있다. 실제 Good design space와 Bad design space 결과를 보면, 연산량이 높음에 따라 최상의 정확도도 높게 나옴을 확인 할 수 있다.

![CDF of FLOPs in different search space, Figure3 ](/assets/images/2020-12-31-MCUNet/figure3.jpg)

#### 1. Resoruce-Constrained Model Specialization  
모든 sub-network을 포함하는 _super network_ 을 학습하도록 한다. sub-network 구조는 모바일 search space에서 가장 많이 사용되는 depthwise convolution 기반으로 구성되는데, 여기에는 kernal size, expansion ratio, variable stage depth 등의 값들이 변수로 추가된다. 즉, super network는 이 모든 값의 최대값으로 설정되어 구성된다. Super network 학습은 전체에 대해 이루어지는 것이 아닌, 4개의 sub-network를 임의로 선택하게 해서 해당 sub-network가 학습되도록 하며 _weight sharing_ 이 적용된다.   

이렇게 super network가 학습되고 나면, evolution 알고리즘을 적용해 best sub-network을 찾는다. 메모리 제약사항을 만족하는 대표 sub-network을 추출하고, 여기에 임의의 확률로 mutation을 적용해 새로운 candidate을 찾고,  정확도를 산출한다. 이러한 과정을 반복적으로 진행한 후, 가장 높은 정확도를 가지는 sub-network을 선택한다.

### __TinyEngine: A Memory-Efficient Inference Library__
TinyEngine은 기본적으로 주어진 리소스에서 inference 속도 최적화를 목적으로 한다. 하지만 기존의 방법과 차이점은 최적화 결과가가 곧 architecture search에 반영되고, 이를 통해 정확도 향상에도 기여한다는 것이다.

크게 4가지 관점에서 최적화가 진행된다.
#### 1.  From interpretation to code generation  
TinyEngine은 기존의 TF-Lite Micro나 CMSIS-NN과 같은 interpreter 기반이 아닌, compiler 단계에서 operation을 코드화 한다. 기존의 방법들은 cross-flatform 개발을 용이하게 하기 위해 이러한 형태로 동작하는데, 그로 인해 추가적인 런타임 메모리가 필요하게 되고, 또한 meta 정보도 저장해야 되는 문제가 있다.   
또한, 모든 operation을 코드화 해서 준비해야 되는 기존 방법과 달리, 주어진 모델에서 사용되는 operation만 코드화 하게 되어 훨씬 경량화를 할 수 있다. 이러한 방법을 통해 TF-Lite Micro나 CMSIS-NN에 비해 4.5-5배 이상 코드 크기를 절감할 수 있었다.
#### 2. Model-adaptive memory scheduling  
기존의 inference 라이브러리들은 각각의 layer별로 메모리 사용을 스케쥴화 하였다. TineEngine은 모델 단위의 statistic을 기반으로 메모리 스케쥴링을 진행하는데, 그 원리는 다음과 같다
1. 전체 layer에 걸쳐 사용할 수 있는 최대 크기 메모리를 찾음
1. 이를 기반으로 각 layer마다 메모리가 가능한 만큼 tiling 한 뒤, 계산 

이러한 방법을 적용하면 최대한 많이 유휴 메모리를 줄이게 되고, 또한 입력 데이터 재사용을 높게하여 불필요한 메모리 파편화와 데이터 이동을 줄일 수 있다.
### 3. Computation kernel specialization
Kernal 연산시 branch instruction overhead을 제거하기 위해, loop unrolling을 적용하고, Conv + Padding + ReLU + BN을 하나의 operation으로 동작하게 하여 속도 향상을 도모 할 수 있다.
### 4. In-place depth-wise convolution
현재 많은 결량화 모델에서 사용되는 Depthwise convolution을 사용하는 것을 전제로 하여 이를 최적화 하였다. Depthwise convolution은 filtering 단계에서 채널간 연산이 이뤄지지 않아, 독립적으로 동작이 가능하다. 즉, 하나의 채널 연산이 끝나면 다른 채널 연산을 위해 값을 덮어 쓰기 할 수 잇고, 이를 통해 Peak memory 사용량을 줄일 수 있다.

## __Experiment__
3가지 데이터 셋에 대해 실험을 진행 하였다.
1. ImageNet
1. Visual Wake Word (VWW), Speech Command
1. Object Detection

### __ImageNet__
MobileNet V2와 Proxyless NAS을 기준으로 성능 비교를 진행하였다. STM32F746 (Cortex-M7, 320kB/1MB Flash)에서 두 모델을 CMSIS-NN을 이용해 최적화를 진행하게 될 때, ImageNet 데이터에 대해 top-1 정확도가 각각 35.2%, 49.5% 정도 나왔는데, TinyEngine을 이용해 최적화를 하면 47.4%, 56.4%의 성능을 도출했다. 여기서 중요한 점은 동일한 모델이 아니라, 동일한 자원 기준이라는 점이다. TinyEngine을 이요하면 훨씬 더 큰 모델을 넣을 수 있기에 좋은 성능이 나온다.

TinyNAS을 이용하면 약 55.5% Top-1 정확도를 가지는 모델을 만들 수 있는데, 여기에 TinyNAS 같이 Co-design 최적화를 진행하여 61.8%의 정확도를 얻게 되었다. Co-design 최적화가 효과가 있음을 보여주는 결과이다.

저자들은 MCU에 주로 사용되는 int8 quantization이 아닌 int4 quantization을 적용한 상태에서 MCUNet 프레임워크를 적용해 보았다. int8에 비해 더 많은 파라미터를 사용할 수 있게 되어서 그런지, 오히려 mixed quantization을 사용한 경우보다 더 높은 성능을 도출 할 수 있었다.

STM32F746 MCU 기준, MCUNet을 통해 만들어진 best model의 top-1 정확도는 70.7%로 상용 MCU 기준 가장 좋은 성능을 보여주었고, 비슷한 정확도를 가지는 ResNet-18, MobileNet V2-0.75 대비하여 메모리 사용량 3.5배, Flash 사용량 5.7배 절약하였다.

### __Visual & Audio Wake Word__
Visual Wake Word (VWW) 와 Goolge Speech Commands 데이터에 대해 각각 테스트를 진행하였다. 이 두 데이터 넷에 대해 _정확도-latency_, _정확도-peak 메모리 사용_ 을 비교하였고, 아래 그림에서 확인 활 수 있듯이, MobileNet V2와 Proxyless NAS에 비해 좋은 성능을 보여준다.

![Accuracy vs. Latency SRAM Memory, Figure9](/assets/images/2020-12-31-MCUNet/figure9.jpg)

### __Object Detection__
Generalization 능력을 평가하기 위해, object detection task에서도 실험을 진행하였다. Object detection은 매우 작은 물체에 대한 위치도 파악해야 되기에, 높은 수준의 resolution을 요구하는데 이는 MCU에 있어서 상당히 어려운 챌린지 중 하나이다.

Pascal VOC에 대하여 Yolo V2을 detector로 사용하고, CMSIS-NN로 최적화한 MobileNet V2와 MCUNet 프레임워크로 만들어진 모델을 비교하였을 때, 전자는 약 31.6% 수준의 mAP을 보여준 반면, 후자는 무려 20% 가까이 향상된 51.4% mAP을 보여주었다.

## __Analysis__
저자들은 MCUNet을 적용하는 데 있어 몇가지 부분에 대해 분석을 진행하였다.
### __Search Space Optimization Matter__
Search space을 매우 크게 설정하였을 때, 오히려 성능이 좋아지지 않음을 확인하였다. 큰 Search space에 당연히 best space가 포함될 확률은 높겠지만, 오히려 super network 학습과 evolution search가 원할히 진행되지 못하였다. 결국, 이러한 분석 결과를 토대로 저자들은 FLOPs이 높은 search space 위주로 선택해서 해당 search space에서 NAS을 수행하는 것이 더 좋다는 것을 간접적으로 말하는 것 같다
### __Per-block Peak Memory Analysis__
네트워크에서 블럭 단위로 activation 크기를 분석하였다. MobileNet V2와 TinyNAS 모델에 대해 비교를 진행하였는데, MobileNet V2에서는 특정 block에서 평균 대비  2.2배 큰 activation 크기를 나타내었다. 이는 제한된 메모리 상황에서 특정 block 때문에 다른 block의 activation 크기를 줄여야 하는 상황이 발생한다. 반명네 TinyNAS의 경우 이 비율이 1.6배 밖에 되지 않았다. 즉, 보다 균형잡힌 메모리 사용을 가능하게 하였다.
### __Sensitivity Analysis on Search Space Optimization__
SRAM 사이즈와 Flash 사이즈를 다양하게 변화시켜 가면서 best width와 resolution을 각각 구해보면, 어떤 영향을 주는지 확인 할 수 있다. 앞서 언급했듯이, SRAM 사이즈는 모델의 크기 그리고 Flash 사이즈는 입력의 resolution에 영향을 주로 준다. 이러한 패턴을 발견하는 것이 그리 쉬운 일이 아니지만, MCUNet을 활용해 보다 정량적으로 분석이 가능하다.
### __Evolution Search__
TinyEngine에서 Evloution search을 적용하였을 때, random search보다 좋은 성능을 보여준다. 반면에 CMSIS-NN에서 Evlolution search을 적용했을 경우는 오히려 random search보다 성능이 떨어지는 것을 보여준다. 이는 메모리 비효율화로 인해 TinyEngine에 대비 작은 모델을 사용할 수 밖에 없었기 때문이다. 결국 TinyEngine은 기존 프레임워크 보다 메모리 사용 관점에서 더 효율적이기에 이러한 결과가 도출되었다고 판단된다.



