---
title:  "[논문 리뷰] Squeeze-and-Excitation Networks"
excerpt: "Squeeze-and-Excitation Networks"

categories:
  - Deep Learning
tags:
  - CNN
  - Vision
classes: wide
last_modified_at: 2020-12-27T00:10:42+09:00
---
__[arxiv link](https://arxiv.org/pdf/1709.01507.pdf)__  

SENet은 convolution features들의 채널간 inter-dependency을 모델링 하기위해 add-on 형태의 네트워크를 제안한 논문이다. 언뜻보면 모델 경량화와 관련이 없는 것처럼 보이지만, 기존의 Network에 미미한 수준의 추가 리소스를 요구하는 SENet 모듈을 추가함으로써, 성능을 비약적으로 향상 시킬 수 있다는 점에서, 본 연구는 경량화와 관련이 있다고 생각 할 수 있다.

## Method
![SENet 구조](/assets/images/2020-12-27-SENet/senet.jpg)
위 그림은 SENet 구조를 도식화한 그림이다. Convolution layer ($F_{tr}$)후에 Squeeze 모듈과 Excitation 모듈을 차례로 적용하고, 이를 통해 생성한 scale vector을 출력 $U$에 적용(channel-wise multiplication)한다. 두 모듈에 대해 설명하자면, 다음과 같다.

### 1. Squeeze: Global information Enbedding
- 채널간의 dependency을 이용하기 위한 채널 aggregation 로직 
- 일반적으로 마지막 convolution layer을 제외하고, 각각의 convoluition layer의 출력들은 대부분 local receptive field로 부터 학습됨 → 하지만 이러한 특징으로 field 밖의 contextual 정보를 이용하기 어려움
- 본 연구에서는 단순히 __Global average pooling (GAP)__ 을 적용하여 squeeze 모듈 구현 (복잡한 구조도 적용 가능하지만, GAP만으로 충분)

### 2. Excitation: Adaptive Recalibration
- 채널간 dependency을 효과적으로 잡기 위해 sigmoid activation을 이용한 간단한 gating 매커니즘 적용
- bottleneck 형태의 두 개의 fully-connected (FC) layer 적용: dimensionality-reduction ratio $r$ 
- Bottlenck 구조의 사용이유는 모델의 complexity을 제한하고, 또한 generalisation에 도움이 되기 위함
- 이렇게 구해진 출력은 convolution layer 출력의 re-scaling 활용: __결국 이는 채널의 self-attention function으로 생각 가능__

하지만, 이렇게 추가되는 모듈로 인해 연산량/메모리 사용량이 많이 늘어난다면, 실제 활용성에서 떨어질 수 밖에 없다. 본 연구에서는 성능과 이러한 리소스 사용량 증가에 대해 매우 좋은 trade-off가 있음을 보여준다.

## Experiments
본 연구에서는 ResNet-50과 SENet이 적용된 SE-ResNet-50과의 비교를 진행하였는데, 우선 연산량 관점에서 보면 다음과 같다
- ResNet-50: 3.86 GFLOPs, SE-ResNet-50: 3.87 GFLOPs (0.26% 증가, $r$=16기준) → 하지만 정확도는 ResNet-101에 근접 (7.58 GFLOPs)
- 실제 CPU inference 시간을 측정해보면, 224x224 이미지 기준으로, ResNet-50: 164 ms, SE-ResNet-50: 167 ms 으로 약간의 속도 저하

파라미터 증가로 인한, 메모리 사용량 관점에서 보면, 
- 2.5 million 증가 (ResNet-50의 10% 수준)
- 하지만, 대부분 마지막 layer (채널이 매우 많음)의 SENet 모듈로 부터 비롯된 것이라 이를 적용하지 않으면 약 4% 증가 (성능 열화는 ImageNet top-5 에러 기준 0.1% 수준)

결국 적절한 SENet 모듈을 기존 CNN 네트워크에 추가해줌으로써, 약간의 추가 리소스 사용으로 높은 성능 향상을 이룰 수 있다는 것이 본 연구에서 주장하는 바이다.

다양한 실험에서 SE Block의 효용성을 확인 할 수 있었다. Image classification, Scene Classification, Object detection 등에서 기존의 ResNet 기반 모델 대비, SE block을 추가한 경우 대부분 성능 향상을 확인 할 수 있었다. (자세한 성능 수치는 논문을 참고)

## Ablation study
본 연구에서는 몇가지 부분에서 ablation study을 진행했는데, 다음과 같다.
- Reduction ratio (r)
  - r 이 크지 않은 경우 의외로 성능의 큰 변화가 보이지 않았다. 이에 r=16을 가장 좋은 balamnce을 보여주었기에 선택 (주어진 모델에 따라 달라질 수 있는 부분이라, 이는 튜닝 포인트)
- Squeeze Operator
  - AvgPool, MaxPool을 비교
  - AvgPool이 조금 더 좋은 성능 향상을 보여주나, MaxPool도 사용가능
- Excitation Operator
  - ReLU, tanh 그리고 sigmoid(현재 사용)을 비교
  - sigmoid가 가장 좋은 성능 보여줌. ReLU의 경우 성능 열화가 심함. Activation function 선택이 중요
- Different stage
  - ResNet-50에서 여러 위치별 SE block을 삽입해 성능 평가
  - 모든 위치에서 성능 향상을 보여줌
- Integration strategy
  - Residual unit의 앞에, 뒤에 혹은 동일한 형태로 삽입
  - 뒤에 삽입한 경우만 성능 열화가 있고, 그외 경우는 준수한 성능을 보여줌